{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52983fad",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "Overview of full text processing pipeline: text input → preprocessing → vectorization → model → output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41383ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'text': [\"I am happy\", \"This is bad\", \"I love it\", \"I hate it\"],\n",
    "    'label': [1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "# Train-test split with stratification for class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'],\n",
    "    test_size=0.5, random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629f10d",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Steps include: tokenization, lowercasing, punctuation removal, stopword removal, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2bb6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sathwik-\n",
      "[nltk_data]     itthagoni/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/sathwik-\n",
      "[nltk_data]     itthagoni/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/sathwik-\n",
      "[nltk_data]     itthagoni/nltk_data...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/sathwik-itthagoni/nltk_data'\n    - '/home/sathwik-itthagoni/miniconda3/envs/nlp_env/nltk_data'\n    - '/home/sathwik-itthagoni/miniconda3/envs/nlp_env/share/nltk_data'\n    - '/home/sathwik-itthagoni/miniconda3/envs/nlp_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCats are running, dogs are barking.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m filtered \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/sathwik-itthagoni/nltk_data'\n    - '/home/sathwik-itthagoni/miniconda3/envs/nlp_env/nltk_data'\n    - '/home/sathwik-itthagoni/miniconda3/envs/nlp_env/share/nltk_data'\n    - '/home/sathwik-itthagoni/miniconda3/envs/nlp_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"Cats are running, dogs are barking.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in filtered]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811efc27",
   "metadata": {},
   "source": [
    "## Assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d2420b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd0c2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6df0885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Replace URLs (http, https, www, or bare domains) with a [LINK] token.\n",
    "    \"\"\"\n",
    "    # Match http(s), www., or bare domain names\n",
    "    url_pattern = re.compile(\n",
    "        r'(http[s]?://\\S+|'         # http or https\n",
    "        r'www\\.\\S+|'                # www.\n",
    "        r'\\b(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}(?:/\\S*)?)'  # domain.com or domain.co.uk/path\n",
    "    )\n",
    "\n",
    "    return re.sub(url_pattern, '[LINK]', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \"\"\"\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51e83326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def rem_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove punctuation from a string.\n",
    "    \"\"\"\n",
    "    exclude = string.punctuation\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf689db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is a test remove punctuationI am in NewYork\n"
     ]
    }
   ],
   "source": [
    "sample_punctuation = \"Hello, world! This is a test: remove punctuation. I am in New-York\"\n",
    "cleaned_text = rem_punctuation(sample_punctuation)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3df21bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from a string.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e23b3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"\n",
    "    Remove emojis from a string.\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "            u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610abacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def replace_emojis(text):\n",
    "    \"\"\"\n",
    "    Remove emojis from a string.\n",
    "    \"\"\"\n",
    "    return emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3feaa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello :smiling_face_with_smiling_eyes:, this is a test with emojis! :rocket:\n"
     ]
    }
   ],
   "source": [
    "sample_emoji_text = \"Hello 😊, this is a test with emojis! 🚀\"\n",
    "sample_emoji_text = remove_emojis(sample_emoji_text)\n",
    "print(sample_emoji_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd727e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence stopwords.\n"
     ]
    }
   ],
   "source": [
    "sample_stopwords = \"This is a sample sentence with some stopwords.\"\n",
    "cleaned_text = remove_stopwords(sample_stopwords)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b68ea8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is a test remove punctuation I am in NewYork\n"
     ]
    }
   ],
   "source": [
    "sample_punctuation = \"Hello, world! This is a test: remove punctuation. I am in New-York\"\n",
    "cleaned_text = rem_punctuation(sample_punctuation)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38ed72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def correct_spelling(text):\n",
    "    \"\"\"\n",
    "    Correct spelling in a string using TextBlob.\n",
    "    \"\"\"\n",
    "    return str(TextBlob(text).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73b801c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that one day this nation will rise up.\n"
     ]
    }
   ],
   "source": [
    "sample_wrong_spelling = \"I havv a dreem that one day this naation will rise up.\"\n",
    "corrected_text = correct_spelling(sample_wrong_spelling)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c5b49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'hi this is a <b>sample</b> text with <i>HTML</i> tags.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93ae613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = 'Check this link: https://example.com and this one: http://test.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddff59a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check this link:  and this one: '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(sample_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "696f71ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi this is a sample text with HTML tags.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70c7434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [tokens.text for tokens in doc ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06c91d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cats', 'are', 'running', ',', 'dogs', 'are', 'barking', '.', 'I', 'am', 'in', 'A.I', 'lab', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_tokenization = \"Cats are running, dogs are barking. I am in A.I lab.\"\n",
    "tokens = tokenize_text(sample_tokenization)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78bdb858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Stem words in a string using PorterStemmer.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0de00c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_word(text):\n",
    "    \"\"\"\n",
    "    Lemmatize a word using WordNetLemmatizer.\n",
    "    \"\"\"\n",
    "    return [word_net_lemmatizer.lemmatize(word,pos='v') for word in tokenize_text(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cfe5ff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cats', 'be', 'run', ',', 'dog', 'be', 'bark', '.', 'I', 'be', 'in', 'A.I', 'lab', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_lemmatization = \"Cats are running, dogs are barking. I am in A.I lab.\"\n",
    "lemmatized_words = lemmatize_word(sample_lemmatization)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7557f6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched page 1\n",
      "Fetched page 2\n",
      "Fetched page 3\n",
      "Attempt 1 failed for page 4: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 4\n",
      "Fetched page 5\n",
      "Fetched page 6\n",
      "Fetched page 7\n",
      "Fetched page 8\n",
      "Fetched page 9\n",
      "Fetched page 10\n",
      "Fetched page 11\n",
      "Attempt 1 failed for page 12: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 12\n",
      "Fetched page 13\n",
      "Fetched page 14\n",
      "Attempt 1 failed for page 15: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 15\n",
      "Fetched page 16\n",
      "Fetched page 17\n",
      "Attempt 1 failed for page 18: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 18: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 18\n",
      "Fetched page 19\n",
      "Fetched page 20\n",
      "Attempt 1 failed for page 21: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 21\n",
      "Fetched page 22\n",
      "Fetched page 23\n",
      "Fetched page 24\n",
      "Fetched page 25\n",
      "Fetched page 26\n",
      "Fetched page 27\n",
      "Fetched page 28\n",
      "Fetched page 29\n",
      "Fetched page 30\n",
      "Fetched page 31\n",
      "Attempt 1 failed for page 32: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 32\n",
      "Attempt 1 failed for page 33: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 33\n",
      "Fetched page 34\n",
      "Fetched page 35\n",
      "Fetched page 36\n",
      "Fetched page 37\n",
      "Fetched page 38\n",
      "Fetched page 39\n",
      "Fetched page 40\n",
      "Fetched page 41\n",
      "Attempt 1 failed for page 42: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 42\n",
      "Fetched page 43\n",
      "Fetched page 44\n",
      "Fetched page 45\n",
      "Attempt 1 failed for page 46: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 46: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 46\n",
      "Fetched page 47\n",
      "Fetched page 48\n",
      "Fetched page 49\n",
      "Fetched page 50\n",
      "Attempt 1 failed for page 51: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 51\n",
      "Fetched page 52\n",
      "Attempt 1 failed for page 53: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 53: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 53\n",
      "Attempt 1 failed for page 54: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 54: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 54\n",
      "Fetched page 55\n",
      "Attempt 1 failed for page 56: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 56\n",
      "Fetched page 57\n",
      "Fetched page 58\n",
      "Fetched page 59\n",
      "Attempt 1 failed for page 60: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 60\n",
      "Attempt 1 failed for page 61: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 61: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 61\n",
      "Fetched page 62\n",
      "Attempt 1 failed for page 63: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 63: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 3 failed for page 63: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 64\n",
      "Fetched page 65\n",
      "Fetched page 66\n",
      "Fetched page 67\n",
      "Fetched page 68\n",
      "Fetched page 69\n",
      "Fetched page 70\n",
      "Fetched page 71\n",
      "Attempt 1 failed for page 72: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 72\n",
      "Fetched page 73\n",
      "Attempt 1 failed for page 74: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 74\n",
      "Fetched page 75\n",
      "Fetched page 76\n",
      "Attempt 1 failed for page 77: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 77\n",
      "Attempt 1 failed for page 78: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 78: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 78\n",
      "Fetched page 79\n",
      "Fetched page 80\n",
      "Fetched page 81\n",
      "Fetched page 82\n",
      "Fetched page 83\n",
      "Fetched page 84\n",
      "Fetched page 85\n",
      "Fetched page 86\n",
      "Fetched page 87\n",
      "Fetched page 88\n",
      "Fetched page 89\n",
      "Attempt 1 failed for page 90: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 90\n",
      "Fetched page 91\n",
      "Fetched page 92\n",
      "Fetched page 93\n",
      "Fetched page 94\n",
      "Fetched page 95\n",
      "Fetched page 96\n",
      "Fetched page 97\n",
      "Fetched page 98\n",
      "Fetched page 99\n",
      "Fetched page 100\n",
      "Attempt 1 failed for page 101: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 101\n",
      "Fetched page 102\n",
      "Fetched page 103\n",
      "Fetched page 104\n",
      "Attempt 1 failed for page 105: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 105: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 105\n",
      "Fetched page 106\n",
      "Fetched page 107\n",
      "Attempt 1 failed for page 108: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 108\n",
      "Attempt 1 failed for page 109: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 109: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 109\n",
      "Fetched page 110\n",
      "Fetched page 111\n",
      "Fetched page 112\n",
      "Fetched page 113\n",
      "Fetched page 114\n",
      "Fetched page 115\n",
      "Fetched page 116\n",
      "Fetched page 117\n",
      "Attempt 1 failed for page 118: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 118\n",
      "Attempt 1 failed for page 119: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 119\n",
      "Fetched page 120\n",
      "Fetched page 121\n",
      "Fetched page 122\n",
      "Fetched page 123\n",
      "Fetched page 124\n",
      "Fetched page 125\n",
      "Fetched page 126\n",
      "Fetched page 127\n",
      "Fetched page 128\n",
      "Fetched page 129\n",
      "Fetched page 130\n",
      "Fetched page 131\n",
      "Fetched page 132\n",
      "Fetched page 133\n",
      "Fetched page 134\n",
      "Fetched page 135\n",
      "Fetched page 136\n",
      "Fetched page 137\n",
      "Attempt 1 failed for page 138: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 138\n",
      "Fetched page 139\n",
      "Fetched page 140\n",
      "Fetched page 141\n",
      "Fetched page 142\n",
      "Fetched page 143\n",
      "Attempt 1 failed for page 144: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 144\n",
      "Attempt 1 failed for page 145: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 145\n",
      "Fetched page 146\n",
      "Fetched page 147\n",
      "Fetched page 148\n",
      "Attempt 1 failed for page 149: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 149: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 3 failed for page 149: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 150\n",
      "Fetched page 151\n",
      "Fetched page 152\n",
      "Fetched page 153\n",
      "Fetched page 154\n",
      "Fetched page 155\n",
      "Fetched page 156\n",
      "Fetched page 157\n",
      "Fetched page 158\n",
      "Fetched page 159\n",
      "Fetched page 160\n",
      "Fetched page 161\n",
      "Fetched page 162\n",
      "Fetched page 163\n",
      "Fetched page 164\n",
      "Fetched page 165\n",
      "Fetched page 166\n",
      "Attempt 1 failed for page 167: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 167\n",
      "Attempt 1 failed for page 168: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 168\n",
      "Fetched page 169\n",
      "Fetched page 170\n",
      "Fetched page 171\n",
      "Fetched page 172\n",
      "Fetched page 173\n",
      "Fetched page 174\n",
      "Fetched page 175\n",
      "Fetched page 176\n",
      "Fetched page 177\n",
      "Fetched page 178\n",
      "Fetched page 179\n",
      "Fetched page 180\n",
      "Attempt 1 failed for page 181: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 181\n",
      "Fetched page 182\n",
      "Fetched page 183\n",
      "Fetched page 184\n",
      "Fetched page 185\n",
      "Attempt 1 failed for page 186: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 186\n",
      "Fetched page 187\n",
      "Fetched page 188\n",
      "Fetched page 189\n",
      "Fetched page 190\n",
      "Fetched page 191\n",
      "Fetched page 192\n",
      "Fetched page 193\n",
      "Fetched page 194\n",
      "Attempt 1 failed for page 195: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 195\n",
      "Fetched page 196\n",
      "Fetched page 197\n",
      "Fetched page 198\n",
      "Attempt 1 failed for page 199: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 199\n",
      "Attempt 1 failed for page 200: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 200\n",
      "Fetched page 201\n",
      "Attempt 1 failed for page 202: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 202\n",
      "Fetched page 203\n",
      "Fetched page 204\n",
      "Fetched page 205\n",
      "Fetched page 206\n",
      "Fetched page 207\n",
      "Attempt 1 failed for page 208: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 208: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 3 failed for page 208: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 209\n",
      "Attempt 1 failed for page 210: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 210\n",
      "Fetched page 211\n",
      "Fetched page 212\n",
      "Fetched page 213\n",
      "Fetched page 214\n",
      "Fetched page 215\n",
      "Fetched page 216\n",
      "Fetched page 217\n",
      "Attempt 1 failed for page 218: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 218: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 218\n",
      "Attempt 1 failed for page 219: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 219\n",
      "Fetched page 220\n",
      "Fetched page 221\n",
      "Attempt 1 failed for page 222: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 222: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 222\n",
      "Attempt 1 failed for page 223: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 223\n",
      "Fetched page 224\n",
      "Fetched page 225\n",
      "Fetched page 226\n",
      "Fetched page 227\n",
      "Attempt 1 failed for page 228: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 228\n",
      "Fetched page 229\n",
      "Fetched page 230\n",
      "Attempt 1 failed for page 231: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 231\n",
      "Fetched page 232\n",
      "Fetched page 233\n",
      "Fetched page 234\n",
      "Fetched page 235\n",
      "Fetched page 236\n",
      "Fetched page 237\n",
      "Fetched page 238\n",
      "Attempt 1 failed for page 239: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 239\n",
      "Fetched page 240\n",
      "Fetched page 241\n",
      "Attempt 1 failed for page 242: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 242\n",
      "Fetched page 243\n",
      "Attempt 1 failed for page 244: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 244\n",
      "Fetched page 245\n",
      "Fetched page 246\n",
      "Attempt 1 failed for page 247: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 247: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 247\n",
      "Fetched page 248\n",
      "Fetched page 249\n",
      "Fetched page 250\n",
      "Fetched page 251\n",
      "Fetched page 252\n",
      "Fetched page 253\n",
      "Fetched page 254\n",
      "Fetched page 255\n",
      "Fetched page 256\n",
      "Fetched page 257\n",
      "Attempt 1 failed for page 258: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 258\n",
      "Fetched page 259\n",
      "Fetched page 260\n",
      "Fetched page 261\n",
      "Attempt 1 failed for page 262: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 262\n",
      "Attempt 1 failed for page 263: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 263\n",
      "Fetched page 264\n",
      "Fetched page 265\n",
      "Fetched page 266\n",
      "Fetched page 267\n",
      "Fetched page 268\n",
      "Fetched page 269\n",
      "Attempt 1 failed for page 270: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 270\n",
      "Fetched page 271\n",
      "Attempt 1 failed for page 272: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 272\n",
      "Fetched page 273\n",
      "Fetched page 274\n",
      "Attempt 1 failed for page 275: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 275\n",
      "Fetched page 276\n",
      "Fetched page 277\n",
      "Fetched page 278\n",
      "Fetched page 279\n",
      "Attempt 1 failed for page 280: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 280\n",
      "Fetched page 281\n",
      "Attempt 1 failed for page 282: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 282\n",
      "Fetched page 283\n",
      "Fetched page 284\n",
      "Fetched page 285\n",
      "Fetched page 286\n",
      "Fetched page 287\n",
      "Fetched page 288\n",
      "Attempt 1 failed for page 289: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 289\n",
      "Fetched page 290\n",
      "Fetched page 291\n",
      "Fetched page 292\n",
      "Fetched page 293\n",
      "Fetched page 294\n",
      "Fetched page 295\n",
      "Fetched page 296\n",
      "Fetched page 297\n",
      "Attempt 1 failed for page 298: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 298\n",
      "Attempt 1 failed for page 299: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 299\n",
      "Fetched page 300\n",
      "Attempt 1 failed for page 301: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 301\n",
      "Fetched page 302\n",
      "Fetched page 303\n",
      "Fetched page 304\n",
      "Fetched page 305\n",
      "Attempt 1 failed for page 306: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 306\n",
      "Fetched page 307\n",
      "Fetched page 308\n",
      "Attempt 1 failed for page 309: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 309\n",
      "Fetched page 310\n",
      "Attempt 1 failed for page 311: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 311\n",
      "Fetched page 312\n",
      "Attempt 1 failed for page 313: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 313\n",
      "Fetched page 314\n",
      "Fetched page 315\n",
      "Fetched page 316\n",
      "Fetched page 317\n",
      "Fetched page 318\n",
      "Attempt 1 failed for page 319: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 319\n",
      "Attempt 1 failed for page 320: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 320: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 320\n",
      "Fetched page 321\n",
      "Fetched page 322\n",
      "Fetched page 323\n",
      "Fetched page 324\n",
      "Attempt 1 failed for page 325: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 325\n",
      "Attempt 1 failed for page 326: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 326\n",
      "Attempt 1 failed for page 327: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 327\n",
      "Fetched page 328\n",
      "Fetched page 329\n",
      "Attempt 1 failed for page 330: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 330\n",
      "Fetched page 331\n",
      "Fetched page 332\n",
      "Fetched page 333\n",
      "Fetched page 334\n",
      "Fetched page 335\n",
      "Fetched page 336\n",
      "Fetched page 337\n",
      "Fetched page 338\n",
      "Fetched page 339\n",
      "Fetched page 340\n",
      "Fetched page 341\n",
      "Fetched page 342\n",
      "Attempt 1 failed for page 343: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 343\n",
      "Attempt 1 failed for page 344: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 344\n",
      "Fetched page 345\n",
      "Fetched page 346\n",
      "Fetched page 347\n",
      "Attempt 1 failed for page 348: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 348\n",
      "Fetched page 349\n",
      "Fetched page 350\n",
      "Fetched page 351\n",
      "Fetched page 352\n",
      "Fetched page 353\n",
      "Fetched page 354\n",
      "Fetched page 355\n",
      "Fetched page 356\n",
      "Fetched page 357\n",
      "Fetched page 358\n",
      "Fetched page 359\n",
      "Fetched page 360\n",
      "Fetched page 361\n",
      "Fetched page 362\n",
      "Fetched page 363\n",
      "Fetched page 364\n",
      "Fetched page 365\n",
      "Fetched page 366\n",
      "Fetched page 367\n",
      "Fetched page 368\n",
      "Fetched page 369\n",
      "Fetched page 370\n",
      "Fetched page 371\n",
      "Fetched page 372\n",
      "Fetched page 373\n",
      "Fetched page 374\n",
      "Attempt 1 failed for page 375: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 375\n",
      "Fetched page 376\n",
      "Fetched page 377\n",
      "Fetched page 378\n",
      "Fetched page 379\n",
      "Fetched page 380\n",
      "Attempt 1 failed for page 381: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 381\n",
      "Fetched page 382\n",
      "Fetched page 383\n",
      "Attempt 1 failed for page 384: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 384: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 384\n",
      "Fetched page 385\n",
      "Fetched page 386\n",
      "Fetched page 387\n",
      "Fetched page 388\n",
      "Fetched page 389\n",
      "Attempt 1 failed for page 390: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 390: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 390\n",
      "Attempt 1 failed for page 391: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 391\n",
      "Fetched page 392\n",
      "Attempt 1 failed for page 393: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 393: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 3 failed for page 393: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 394\n",
      "Fetched page 395\n",
      "Fetched page 396\n",
      "Fetched page 397\n",
      "Fetched page 398\n",
      "Fetched page 399\n",
      "Attempt 1 failed for page 400: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 400\n",
      "Fetched page 401\n",
      "Fetched page 402\n",
      "Fetched page 403\n",
      "Fetched page 404\n",
      "Fetched page 405\n",
      "Fetched page 406\n",
      "Fetched page 407\n",
      "Fetched page 408\n",
      "Attempt 1 failed for page 409: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 409\n",
      "Attempt 1 failed for page 410: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 410\n",
      "Fetched page 411\n",
      "Fetched page 412\n",
      "Fetched page 413\n",
      "Fetched page 414\n",
      "Fetched page 415\n",
      "Fetched page 416\n",
      "Fetched page 417\n",
      "Fetched page 418\n",
      "Fetched page 419\n",
      "Attempt 1 failed for page 420: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 420\n",
      "Fetched page 421\n",
      "Fetched page 422\n",
      "Fetched page 423\n",
      "Fetched page 424\n",
      "Fetched page 425\n",
      "Fetched page 426\n",
      "Fetched page 427\n",
      "Attempt 1 failed for page 428: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 428: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 428\n",
      "Fetched page 429\n",
      "Attempt 1 failed for page 430: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 430\n",
      "Attempt 1 failed for page 431: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 431: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 431\n",
      "Fetched page 432\n",
      "Attempt 1 failed for page 433: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 433\n",
      "Fetched page 434\n",
      "Attempt 1 failed for page 435: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 435\n",
      "Fetched page 436\n",
      "Attempt 1 failed for page 437: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 437\n",
      "Fetched page 438\n",
      "Fetched page 439\n",
      "Attempt 1 failed for page 440: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 440\n",
      "Fetched page 441\n",
      "Fetched page 442\n",
      "Fetched page 443\n",
      "Attempt 1 failed for page 444: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 444\n",
      "Fetched page 445\n",
      "Fetched page 446\n",
      "Fetched page 447\n",
      "Fetched page 448\n",
      "Attempt 1 failed for page 449: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 449\n",
      "Fetched page 450\n",
      "Fetched page 451\n",
      "Fetched page 452\n",
      "Attempt 1 failed for page 453: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 453\n",
      "Fetched page 454\n",
      "Fetched page 455\n",
      "Fetched page 456\n",
      "Fetched page 457\n",
      "Fetched page 458\n",
      "Attempt 1 failed for page 459: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 459\n",
      "Fetched page 460\n",
      "Fetched page 461\n",
      "Attempt 1 failed for page 462: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 462\n",
      "Fetched page 463\n",
      "Fetched page 464\n",
      "Fetched page 465\n",
      "Fetched page 466\n",
      "Fetched page 467\n",
      "Fetched page 468\n",
      "Fetched page 469\n",
      "Attempt 1 failed for page 470: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 470\n",
      "Attempt 1 failed for page 471: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Attempt 2 failed for page 471: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Fetched page 471\n",
      "                       name  \\\n",
      "0  The Shawshank Redemption   \n",
      "1             The Godfather   \n",
      "2     The Godfather Part II   \n",
      "3          Schindler's List   \n",
      "4              12 Angry Men   \n",
      "\n",
      "                                         description            genre  \n",
      "0  Imprisoned in the 1940s for the double murder ...         [18, 80]  \n",
      "1  Spanning the years 1945 to 1955, a chronicle o...         [18, 80]  \n",
      "2  In the continuing saga of the Corleone crime f...         [18, 80]  \n",
      "3  The true story of how businessman Oskar Schind...  [18, 36, 10752]  \n",
      "4  The defense and the prosecution have rested an...             [18]  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://api.themoviedb.org/3/movie/top_rated\"\n",
    "api_key = \"8265bd1679663a7ea12ac168da84d2e8\"\n",
    "language = \"en-US\"\n",
    "all_movies = []\n",
    "\n",
    "for page in range(1, 472):\n",
    "    url = f\"{base_url}?api_key={api_key}&language={language}&page={page}\"\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                movies = [\n",
    "                    {\n",
    "                        \"name\": movie.get(\"title\"),\n",
    "                        \"description\": movie.get(\"overview\"),\n",
    "                        \"genre\": movie.get(\"genre_ids\")\n",
    "                    }\n",
    "                    for movie in data.get(\"results\", [])\n",
    "                ]\n",
    "                all_movies.extend(movies)\n",
    "                print(f\"Fetched page {page}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Page {page} failed with status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed for page {page}: {e}\")\n",
    "            time.sleep(1)\n",
    "    time.sleep(0.3)  # Sleep to avoid rate limiting\n",
    "\n",
    "# Create DataFrame\n",
    "all_movies_df = pd.DataFrame(all_movies)\n",
    "print(all_movies_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcd427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e2ad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [18, 80]\n",
       "1                   [18, 80]\n",
       "2                   [18, 80]\n",
       "3            [18, 36, 10752]\n",
       "4                       [18]\n",
       "                ...         \n",
       "9335           [18, 53, 878]\n",
       "9336    [14, 53, 28, 12, 27]\n",
       "9337                [35, 27]\n",
       "9338          [27, 9648, 14]\n",
       "9339                [35, 27]\n",
       "Name: genre, Length: 9340, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_movies_df['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdaa7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.themoviedb.org/3/genre/movie/list?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US\"  # Replace with the actual URL\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Assuming the data is in JSON format\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract relevant fields (adjust keys based on the actual structure of the JSON)\n",
    "    genre_mapping = {\n",
    "        genre['id']: genre['name'] for genre in data.get(\"genres\", [])\n",
    "    }\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaedb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies_df['genre'] = all_movies_df['genre'].apply(\n",
    "    lambda x: ', '.join(genre_mapping.get(genre_id, 'Unknown') for genre_id in x) if isinstance(x, list) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef56de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "    lambda x: x.lower() if isinstance(x, str) else \"No description available\" \n",
    ")           #FIRST CONVERT TO LOWER CASE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53008fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   remove_html_tags\n",
    ")           #FIRST CONVERT TO LOWER CASE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "63902db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   remove_urls\n",
    ")           #FIRST CONVERT TO LOWER CASE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9117984",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   rem_punctuation\n",
    ")           #FIRST CONVERT TO LOWER CASE \n",
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   remove_stopwords\n",
    ")           #FIRST CONVERT TO LOWER CASE \n",
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   correct_spelling\n",
    ")           #FIRST CONVERT TO LOWER CASE \n",
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   remove_html_tags\n",
    ")           #FIRST CONVERT TO LOWER CASE \n",
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   tokenize_text\n",
    ")           #FIRST CONVERT TO LOWER CASE \n",
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "  stem_text\n",
    ")           #FIRST CONVERT TO LOWER CASE \n",
    "\n",
    "all_movies_df['description'] = all_movies_df['description'].apply(\n",
    "   lemmatize_word\n",
    ")           #FIRST CONVERT TO LOWER CASE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be4498",
   "metadata": {},
   "source": [
    "## Bag of Words & TF-IDF\n",
    "Vectorizing text using BoW and TF-IDF using sklearn. Comparing sparsity and feature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24da8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "docs = [\"I love NLP\", \"NLP is fun\", \"I hate bugs\"]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(docs)\n",
    "print(\"BoW features:\", cv.get_feature_names_out(), bow.toarray())\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_vec = tfidf.fit_transform(bow)\n",
    "print(\"TF-IDF array:\\n\", tfidf_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefee76a",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Training CBOW and Skip-Gram models using Gensim or PyTorch. Visualizing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"NLP\"], [\"NLP\", \"is\", \"fun\"], [\"we\", \"love\", \"learning\"]]\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, epochs=50)\n",
    "print(model.wv[\"nlp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f812a",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "Using logistic regression or a neural network on BoW/TF-IDF vectors or embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cats = ['alt.atheism', 'soc.religion.christian']\n",
    "data = fetch_20newsgroups(subset='train', categories=cats, shuffle=True, random_state=42)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=cats, shuffle=True, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline.fit(data.data, data.target)\n",
    "pred = pipeline.predict(data_test.data)\n",
    "print(classification_report(data_test.target, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
