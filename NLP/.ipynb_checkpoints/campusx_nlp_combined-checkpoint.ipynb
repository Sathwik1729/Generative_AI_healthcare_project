{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52983fad",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "Overview of full text processing pipeline: text input → preprocessing → vectorization → model → output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41383ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'text': [\"I am happy\", \"This is bad\", \"I love it\", \"I hate it\"],\n",
    "    'label': [1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.5, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629f10d",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Steps include: tokenization, lowercasing, punctuation removal, stopword removal, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2bb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"Cats are running, dogs are barking.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in filtered]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be4498",
   "metadata": {},
   "source": [
    "## Bag of Words & TF-IDF\n",
    "Vectorizing text using BoW and TF-IDF using sklearn. Comparing sparsity and feature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24da8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "docs = [\"I love NLP\", \"NLP is fun\", \"I hate bugs\"]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(docs)\n",
    "print(\"BoW features:\", cv.get_feature_names_out(), bow.toarray())\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_vec = tfidf.fit_transform(bow)\n",
    "print(\"TF-IDF array:\\n\", tfidf_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefee76a",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Training CBOW and Skip-Gram models using Gensim or PyTorch. Visualizing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"NLP\"], [\"NLP\", \"is\", \"fun\"], [\"we\", \"love\", \"learning\"]]\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, epochs=50)\n",
    "print(model.wv[\"nlp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f812a",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "Using logistic regression or a neural network on BoW/TF-IDF vectors or embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cats = ['alt.atheism', 'soc.religion.christian']\n",
    "data = fetch_20newsgroups(subset='train', categories=cats, shuffle=True, random_state=42)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=cats, shuffle=True, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline.fit(data.data, data.target)\n",
    "pred = pipeline.predict(data_test.data)\n",
    "print(classification_report(data_test.target, pred))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}